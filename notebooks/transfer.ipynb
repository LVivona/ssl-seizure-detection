{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load Model\n",
    "Before we begin transfer learning we first have to load the model. This can be done in two ways (1) load the `model.pth` which includes the model's architecture and weights, or (2) load the model class itself, defined in `pyg_model.py`. Either way works, but (2) is a bit safer when dealing with unknown files. After loading the model, we then load the state dictionary `model_state_dict.pth` which allows us to reference specific layers of the model and is crucial for examining, extracting, or modifying its underlying architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xaviermootoo/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym\n",
      "  warnings.warn(\"Could not define global config object. Please install \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temporal_shuffling(\n",
       "  (encoder): gnn_encoder(\n",
       "    (edge_mlp): EdgeMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=576, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (conv1): NNConv(9, 64, aggr=add, nn=EdgeMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=64, out_features=576, bias=True)\n",
       "      )\n",
       "    ))\n",
       "    (conv2): GATConv(64, 32, heads=1)\n",
       "    (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# Mac\n",
    "model_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/models/jh101/model/temporal_shuffling.pth\"\n",
    "model_dict_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/models/jh101/model/temporal_shuffling_state_dict.pth\"\n",
    "\n",
    "# Load model\n",
    "model = torch.load(model_path)\n",
    "\n",
    "# Load state dictionary\n",
    "model_dict = torch.load(model_dict_path)\n",
    "\n",
    "# Set the state dictionary to the model\n",
    "model.load_state_dict(model_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Extract Layers\n",
    "In this step we extract the layers we want to use for the supervised model downstream. In this case, we need the NNConv and GATConv layers from our model, but since our NNConv actually depends on a separate layer called EdgeMLP (which is just a multilayer perpcetron), we'll need that too, since it's essentially part of the NNConv layer's parameters. You can assign it the old fashioned way using `EdgeMLP_module = model.edge_mlp` but this will create issues later on when we try to make two copies of `EdgeMLP_module` for freezing and unfreezing it, so we use the `copy` package instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EdgeMLP_pretrained = copy.deepcopy(model.encoder.edge_mlp)\n",
    "NNConv_pretrained = copy.deepcopy(model.encoder.conv1)\n",
    "GATConv_pretrained = copy.deepcopy(model.encoder.conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the weights of a layer with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp.0.weight \t torch.Size([128, 3])\n",
      "mlp.0.bias \t torch.Size([128])\n",
      "mlp.2.weight \t torch.Size([64, 128])\n",
      "mlp.2.bias \t torch.Size([64])\n",
      "mlp.4.weight \t torch.Size([576, 64])\n",
      "mlp.4.bias \t torch.Size([576])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in EdgeMLP_pretrained.state_dict():\n",
    "    print(param_tensor, \"\\t\", EdgeMLP_pretrained.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a test running some random input through the EdgeMLP, to verify it's functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0321,  0.0009,  0.0084,  ...,  0.0310, -0.0705,  0.0445],\n",
      "        [-0.0479,  0.0043, -0.0925,  ...,  0.0205, -0.1029,  0.1922],\n",
      "        [-0.0017, -0.1137, -0.0789,  ...,  0.1040, -0.0402,  0.0833],\n",
      "        ...,\n",
      "        [-0.0005, -0.0056,  0.0161,  ...,  0.0091, -0.0817,  0.0355],\n",
      "        [-0.0350, -0.0275, -0.0349,  ...,  0.0347, -0.0643,  0.1101],\n",
      "        [-0.1118,  0.0496, -0.0055,  ...,  0.0199,  0.0280,  0.0592]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create some dummy data\n",
    "dummy_edge_attr = torch.randn(10, 3)  # 10 edges, each with `num_edge_features` features\n",
    "\n",
    "# Run the data through the `edge_mlp` layer\n",
    "output = EdgeMLP_pretrained(dummy_edge_attr)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Downstream Task\n",
    "After extracting the layers and verifying everything is functional, we can now either (1) use the layers and their weights as initialization, or (2) use the layers but freeze the weights (i.e. they won't be updated during training). Below uses method (1), using our transferred layers as the initial layers of our network, and then we add on newer (untrained) layers on top of it. I've opted to use another `NNConv` and `GATConv` layer from `PyG`, adding onto the existing `NNConv` and `GATConv` layers, as well as a `global_mean_pool` layer and two fully connected layers. Now we're ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import supervised_downstream1\n",
    "config = {\n",
    "    \"hidden_channels\": [64, 64, 32],\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "pretrained_layers = [EdgeMLP_pretrained, NNConv_pretrained, GATConv_pretrained]\n",
    "\n",
    "model = supervised_downstream1(config, pretrained_layers, frozen=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1, Parameter: bias, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.0.weight, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.0.bias, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.2.weight, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.2.bias, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.4.weight, Frozen: True\n",
      "Layer: conv1, Parameter: nn.mlp.4.bias, Frozen: True\n",
      "Layer: conv1, Parameter: lin.weight, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.0.weight, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.0.bias, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.2.weight, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.2.bias, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.4.weight, Frozen: True\n",
      "Layer: conv1, Parameter: edge_mlp.mlp.4.bias, Frozen: True\n",
      "Layer: conv2, Parameter: att_src, Frozen: True\n",
      "Layer: conv2, Parameter: att_dst, Frozen: True\n",
      "Layer: conv2, Parameter: bias, Frozen: True\n",
      "Layer: conv2, Parameter: lin_src.weight, Frozen: True\n"
     ]
    }
   ],
   "source": [
    "def check_frozen_status(model):\n",
    "    layers_to_check = [\"conv1\", \"conv2\"]  # Names of the layers in your model that are pretrained\n",
    "\n",
    "    for layer_name in layers_to_check:\n",
    "        layer = getattr(model, layer_name)\n",
    "        for name, param in layer.named_parameters():\n",
    "            print(f\"Layer: {layer_name}, Parameter: {name}, Frozen: {not param.requires_grad}\")\n",
    "\n",
    "# Check if the pretrained layers are frozen or not\n",
    "check_frozen_status(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning on Downstream Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in dataset: 1113.\n",
      "Total number of examples used: 1113.\n",
      "Number of training examples: 890. Number of training batches: 28.\n",
      "Number of validation examples: 223. Number of validation batches: 7.\n",
      "Number of test examples: 112. Number of test batches: 4.\n"
     ]
    }
   ],
   "source": [
    "from preprocess import create_data_loaders\n",
    "\n",
    "# Paths\n",
    "data_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/patient_pyg/jh101/supervised/jh101_run1.pt\"\n",
    "data = torch.load(data_path)\n",
    "\n",
    "loaders, _ = create_data_loaders(data, data_size=1.0, val_ratio=0.2, test_ratio=0.1, batch_size=32, num_workers=4, model_id=\"supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Model output: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = loaders\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"Model output: {model(batch).size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Transfer\n",
    "If you want to do all of the above in one step, see below. Note that this implemented in `train.py` when you select the `downstream1` or `downstream2` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\Anaconda3\\envs\\torch2_cuda11.8\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from preprocess import extract_layers\n",
    "from models import downstream1, downstream2\n",
    "\n",
    "# Mac\n",
    "model_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/models/jh101/model/temporal_shuffling.pth\"\n",
    "model_dict_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/models/jh101/model/temporal_shuffling_state_dict.pth\"\n",
    "\n",
    "# PC\n",
    "model_path = r\"C:\\Users\\xmoot\\Desktop\\Models\\ssl-seizure-detection\\jh101\\model\\temporal_shuffling.pth\"\n",
    "model_dict_path = r\"C:\\Users\\xmoot\\Desktop\\Models\\ssl-seizure-detection\\jh101\\model\\temporal_shuffling_state_dict.pth\"\n",
    "\n",
    "extracted_layers = extract_layers(model_path, model_dict_path, \"temporal_shuffling\")\n",
    "\n",
    "config1 = {\n",
    "    \"hidden_channels\": [64, 64, 32],\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    \"hidden_channels\": 32,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model1 = downstream1(config1, extracted_layers, frozen=False).to(device)\n",
    "model2 = downstream2(config2, extracted_layers, frozen=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data for supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples in dataset: 1113.\n",
      "Total number of examples used: 1113.\n",
      "Number of training examples: 890. Number of training batches: 28.\n",
      "Number of validation examples: 223. Number of validation batches: 7.\n",
      "Number of test examples: 112. Number of test batches: 4.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "from preprocess import create_data_loaders\n",
    "\n",
    "# Mac\n",
    "data_path = \"/Users/xaviermootoo/Documents/Data/ssl-seizure-detection/patient_pyg/jh101/supervised/jh101_run1.pt\"\n",
    "\n",
    "# PC\n",
    "data_path = r\"C:\\Users\\xmoot\\Desktop\\Data\\ssl-seizure-detection\\patient_pyg\\jh101\\supervised\\jh101_run1.pt\"\n",
    "\n",
    "\n",
    "data = torch.load(data_path)\n",
    "loaders, _ = create_data_loaders(data, data_size=0.1, val_ratio=0.2, test_ratio=0.1, batch_size=32, num_workers=4, model_id=\"supervised\")\n",
    "train_loader = loaders[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the model output is working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: tensor([5.5096, 3.2366, 6.4307, 3.9484, 4.0148, 4.6887, 5.0815, 4.0649, 4.1764,\n",
      "        3.5129, 5.6768, 6.5692, 3.6030, 4.9131, 2.8050, 3.2610, 3.0446, 2.3526,\n",
      "        6.0145, 1.4550, 7.0167, 5.3131, 4.2521, 3.1954, 4.2855, 3.9693, 4.2903,\n",
      "        2.9354, 2.5056, 4.2362, 5.4698, 3.6607], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Model Output: tensor([0.9967, 0.5494, 1.3112, 0.5155, 0.8721, 0.7237, 0.7408, 0.8231, 0.8585,\n",
      "        0.9748, 0.8397, 0.9853, 0.4972, 0.6344, 0.6492, 0.8180, 1.0487, 0.8021,\n",
      "        0.9034, 0.6751, 0.8598, 0.9663, 0.9806, 0.8835, 1.2074, 0.6641, 1.3122,\n",
      "        0.7555, 0.9694, 0.6843, 0.7117, 0.7173], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    print(f\"Model Output: {model1(batch)}\")\n",
    "    print(f\"Model Output: {model2(batch)}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_cuda11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
