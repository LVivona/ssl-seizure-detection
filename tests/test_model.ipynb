{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import relative_positioning_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node features of shape (num_nodes, num_node_features) and type torch.float32\n",
    "from torch_geometric.loader import DataLoader\n",
    "from preprocess import PairData\n",
    "\n",
    "x1 = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                    [2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "                    [3, 3, 3, 3, 3, 3, 3, 3, 3]], dtype=torch.float32)\n",
    "x2 = torch.tensor([[4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "                    [5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
    "                    [6, 6, 6, 6, 6, 6, 6, 6, 6]], dtype=torch.float32)\n",
    "\n",
    "# Edge indices of shape (2, num_edges) and type torch.long\n",
    "edge_index1 = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "                             [1, 0, 2, 1, 3, 2]], dtype=torch.long)\n",
    "edge_index2 = torch.tensor([[0, 1, 1, 2],\n",
    "                             [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "# Edge features of shape (num_edges, num_edge_features) and type torch.float32\n",
    "edge_attr1 = torch.tensor([[0, 0, 0],\n",
    "                            [1,1,1],\n",
    "                            [2,2,2],\n",
    "                            [3,3,3],\n",
    "                            [4,4,4],\n",
    "                            [5,5,5]], dtype=torch.float32)\n",
    "edge_attr2 = torch.tensor([[6,6,6],\n",
    "                            [7,7,7],\n",
    "                            [8,8,8],\n",
    "                            [9,9,9]], dtype=torch.float32)\n",
    "\n",
    "# Pair label of shape (1,) and type torch.long\n",
    "y = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "data = PairData(x1=x1, edge_index1=edge_index1, edge_attr1=edge_attr1,  # Graph 1.\n",
    "                x2=x2, edge_index2=edge_index2, edge_attr2=edge_attr2,  # Graph 2.\n",
    "                y=y) #Graph pair label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data, data, data, data]\n",
    "dataloader = DataLoader(data_list, batch_size=2, follow_batch=['x1', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (12x1 and 3x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m batch\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39;49mx1, batch\u001b[39m.\u001b[39;49medge_index1, batch\u001b[39m.\u001b[39;49medge_attr1, batch\u001b[39m.\u001b[39;49mx1_batch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             batch\u001b[39m.\u001b[39;49mx2, batch\u001b[39m.\u001b[39;49medge_index2, batch\u001b[39m.\u001b[39;49medge_attr2, batch\u001b[39m.\u001b[39;49mx2_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, batch\u001b[39m.\u001b[39my)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/xaviermootoo/Documents/VScode/ssl-seizure-detection/tests/test_model.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VScode/ssl-seizure-detection/tests/../src/models.py:76\u001b[0m, in \u001b[0;36mrelative_positioning_new.forward\u001b[0;34m(self, x1, edge_index1, edge_attr1, batch1, x2, edge_index2, edge_attr2, batch2, mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1, edge_index1, edge_attr1, batch1, x2, edge_index2, edge_attr2, batch2, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     75\u001b[0m     \u001b[39m# First graph's embeddings\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     z1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x1, edge_index1, edge_attr1, batch1)\n\u001b[1;32m     78\u001b[0m     \u001b[39m# Second graph's embeddings\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x2, edge_index2, edge_attr2, batch2)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VScode/ssl-seizure-detection/tests/../src/models.py:43\u001b[0m, in \u001b[0;36mgnn_encoder.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr, batch):\n\u001b[1;32m     42\u001b[0m     \u001b[39m# NNConv layer\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_attr))\n\u001b[1;32m     45\u001b[0m     \u001b[39m# GATConv layer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:102\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m     99\u001b[0m     x: OptPairTensor \u001b[39m=\u001b[39m (x, x)\n\u001b[1;32m    101\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_attr\u001b[39m=\u001b[39;49medge_attr, size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    104\u001b[0m x_r \u001b[39m=\u001b[39m x[\u001b[39m1\u001b[39m]\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m x_r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:467\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[0;32m--> 467\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmsg_kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    469\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:114\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmessage\u001b[39m(\u001b[39mself\u001b[39m, x_j: Tensor, edge_attr: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn(edge_attr)\n\u001b[1;32m    115\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_channels_l, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels)\n\u001b[1;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmatmul(x_j\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), weight)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VScode/ssl-seizure-detection/tests/../src/models.py:20\u001b[0m, in \u001b[0;36mEdgeMLP.forward\u001b[0;34m(self, edge_attr)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, edge_attr):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(edge_attr)\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch2_mps/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x1 and 3x128)"
     ]
    }
   ],
   "source": [
    "model = relative_positioning_new(num_node_features=3, num_edge_features=3, hidden_channels=[32, 64], out_channels=128)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for batch in dataloader:\n",
    "    # Move batch data to the device\n",
    "    batch.x1 = batch.x1.to(device)\n",
    "    batch.edge_index1 = batch.edge_index1.to(device)\n",
    "    batch.edge_attr1 = batch.edge_attr1.to(device)\n",
    "    batch.x1_batch = batch.x1_batch.to(device)\n",
    "    \n",
    "    batch.x2 = batch.x2.to(device)\n",
    "    batch.edge_index2 = batch.edge_index2.to(device)\n",
    "    batch.edge_attr2 = batch.edge_attr2.to(device)\n",
    "    batch.x2_batch = batch.x2_batch.to(device)\n",
    "    \n",
    "    batch.y = batch.y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(batch.x1, batch.edge_index1, batch.edge_attr1, batch.x1_batch,\n",
    "                batch.x2, batch.edge_index2, batch.edge_attr2, batch.x2_batch)\n",
    "    loss = criterion(out, batch.y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2_mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
